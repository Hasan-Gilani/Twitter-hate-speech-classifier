{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24783.000000\n",
       "mean         1.110277\n",
       "std          0.462089\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          2.000000\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "datadir = '/'.join(cwd.split('/')) + '/data/'\n",
    "data = pd.read_csv(datadir + 'labeled_data.csv', sep=',',index_col=0)\n",
    "data['class'].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "def clean_twitter(data):\n",
    "    tweets=data['tweet']\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    cleandata = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet.split()\n",
    "        cleanstring = []\n",
    "        for o in tokens:\n",
    "            # Clearn words that start as @\n",
    "            if o.startswith('@'): continue\n",
    "            # remove punctuation from each token\n",
    "            o = o.translate(table)  \n",
    "            # filter out short tokens \n",
    "            if len(o) < 2: continue\n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            if not o.isalpha(): continue\n",
    "            # filter out stop words\n",
    "            if o in stop_words: continue\n",
    "            o = lemma.lemmatize(o)\n",
    "            # change to lowercase\n",
    "            o = o.lower()\n",
    "            cleanstring.append(o)\n",
    "        #convert the array to string\n",
    "        cleanstring=' '.join(cleanstring)\n",
    "        cleandata.append(cleanstring)\n",
    "    return cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test data\n",
    "#need to be modified to cover the whole dataset!!!\n",
    "raw_data=data[:5000]\n",
    "twitter_cleaned=clean_twitter(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output matrix\n",
    "\n",
    "def prepare_data(data, mode):\n",
    "\t# create the tokenizer\n",
    "\ttokenizer = Tokenizer()\n",
    "\t# fit the tokenizer on the documents\n",
    "\ttokenizer.fit_on_texts(data)\n",
    "\t# encode training data set\n",
    "\tdata_m = tokenizer.texts_to_matrix(data, mode=mode)\n",
    "\treturn data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the data into matrix\n",
    "X = prepare_data(twitter_cleaned, 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=raw_data['class']\n",
    "kfolds = KFold(raw_data.shape[0], n_folds = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=5000, n_folds=4, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'solver': ['newton-cg'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2'], 'multi_class': ['multinomial']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='neg_log_loss',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_lr = {'C':[10**i for i in range(-3, 3)], 'penalty':[ 'l2'], 'multi_class':['multinomial'], 'solver':['newton-cg']}\n",
    "\n",
    "#2nd, call the GridSearchCV class, use LogisticRegression and 'roc_auc' for scoring\n",
    "lr_grid_search = GridSearchCV(LogisticRegression(), param_grid_lr, cv = kfolds,scoring='neg_log_loss') \n",
    "lr_grid_search.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     1\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "      ..\n",
       "478    1\n",
       "479    1\n",
       "480    1\n",
       "481    2\n",
       "482    1\n",
       "483    1\n",
       "484    1\n",
       "485    1\n",
       "486    1\n",
       "487    1\n",
       "488    1\n",
       "489    1\n",
       "490    1\n",
       "491    1\n",
       "492    1\n",
       "493    1\n",
       "494    1\n",
       "495    1\n",
       "496    1\n",
       "497    1\n",
       "498    1\n",
       "499    1\n",
       "500    1\n",
       "501    1\n",
       "502    2\n",
       "503    1\n",
       "504    1\n",
       "505    1\n",
       "506    1\n",
       "507    1\n",
       "Name: class, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_1 = lr_grid_search.best_score_\n",
    "#number of obs to be test !!!\n",
    "test_index=int (1/10*raw_data.shape[0])\n",
    "#print(best_1)\n",
    "X_train = X[test_index:]\n",
    "Y_train = Y[test_index:]\n",
    "X_test=X[:test_index]\n",
    "Y_test=Y[:test_index]\n",
    "\n",
    "lr_best = LogisticRegression(**lr_grid_search.best_params_ ) \n",
    "lr_best.fit(X_train, Y_train)\n",
    "pred_lr=lr_best.predict(X_test)\n",
    "pred_lr\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def printreport(pred, true)\n",
    "y_true = true\n",
    "y_pred = pred\n",
    "target_names = ['class 2', 'class 1', 'class 0']\n",
    "print (metrics.confusion_matrix(y_true, y_pred ), labels=target_names)\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid_svm = {}\n",
    "svm_grid_search = GridSearchCV(svm.SVC(kernel='rbf',decision_function_shape='ovo',probability=True), param_grid_svm, cv=kfolds, scoring='neg_log_loss')\n",
    "svm_grid_search.fit(X, Y)\n",
    "svm_grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3982564861800524\n"
     ]
    }
   ],
   "source": [
    "print (svm_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "\t# prepare data for mode\n",
    "\tXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "\t# evaluate model on data for mode\n",
    "\tresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
